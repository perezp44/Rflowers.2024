{
  "hash": "baf2c1d44da89b194f916d6d2d812f67",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Utilidades para la docencia\"\nsubtitle: | \n   Algunos scripts/app's/pkgs/ con utilidad para la docencia/investigación\ndate: 2024-03-08\ndraft: true\ncategories: [docencia, recursos]\nimage: \"imagenes/img_datos_03.png\"\n#code-line-numbers: false\nexecute:\n  eval: false\n---\n\n\nEn este TBit recopilo scripts/app's/pkgs/ con potencial utilidad para la docencia/investigación.\n\n\n## Bibliometrics\n\n\n### OpenAlexR pkg\n\n[OpenAlexR](https://docs.ropensci.org/openalexR/) es un pkg de R que permite acceder a la API de [OpenAlex](https://openalex.org/) to retrieve **bibliographic infomation** about publications, authors, venues, institutions and concepts. \n\nOpenAlex es una base de datos bibliográficos de acceso libre, creada en 2022 que, según nos cuentan [aquí](https://www.editorialteseo.com/archivos/33215/los-libros-y-las-revistas-cientificas-de-teseo-indexadas-en-openalex/), ha logrado reunir más de 250 millones de textos de múltiples disciplinas. \nOpenAlex, a veces es presentada como la base de datos académica que puede sustituir a otras bases de datos bibliográficas. [Aquí](https://www.lluiscodina.com/openalex-scopus/) analizan si realmente OpenAlex es una alternativa a otras bases de datos como Scopus o Web of Science.\n\n\nPara entender un poco qué es OpenAlex, [aquí](https://instituciones.sld.cu/faenflidiadoce/2023/06/02/catalogo-openalex/) y [aquí](https://www.lluiscodina.com/openalex-scopus/) tienes información.\n\n\nComo todas las bases de datos bibliometrícas tienen problemas, por ejemplo yo no estoy como autor ... todavía, pero algunos de mis trabajos están adjudicados a otro Pedro. \n\nEs recomendable ejecutar esta instrucción de R: `options(openalexR.mailto = \"tu-email@uv.es\")` para si la API tiene mucha carga,  proporcionar a OpenAlex un email to enter the polite pool.\n\n\nAlgunas queries que se pueden hacer:\n\n- Das un vector de DOI's de trabajos y te devuelve la información bibliográfica de autores, revista, etc.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#- works from DOI's\nworks_from_dois <- oa_fetch(\n  entity = \"works\",\n  doi = c(\"10.1016/j.joi.2017.08.007\", \"https://doi.org/10.1007/s11192-013-1221-3\"),\n  verbose = TRUE)\n```\n:::\n\n\n\n- Descargar todos los trabajos de un vector de autores\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#- Goal: Download all works published by a set of authors (known ORCIDs).\nworks_from_orcids <- oa_fetch(\n  entity = \"works\",\n  author.orcid = c(\"0000-0001-6187-6610\", \"0000-0002-8517-9411\"),\n  verbose = TRUE\n)\n```\n:::\n\n\n- Obtener listado de trabajos sobre un determinado tópico que hayan sido citados mas de 50 veces\n\n\n::: {.cell}\n\n```{.r .cell-code}\nworks_search2 <- oa_fetch(\n  entity = \"works\",\n  title.search = c(\"gender gap\", \"awards\"),\n  cited_by_count = \">50\",\n  from_publication_date = \"2010-01-01\",\n  to_publication_date = \"2024-12-31\",\n  options = list(sort = \"cited_by_count:desc\"),\n  verbose = TRUE\n)\n```\n:::\n\n\n- En la web del paquete hay más ejemplos. Por ejemplo [aquí](https://docs.ropensci.org/openalexR/#-example-analyses) se hace un análisis de los tópicos/concepts tratado en el tiempo\n\n- El chunk de abajo ordena las instituciones españolas en función del número de citas \n\n\n::: {.cell}\n\n```{.r .cell-code}\nspain_institutions <- oa_fetch(\n  entity = \"institutions\",\n  country_code = \"es\",\n  type = \"education\",\n  verbose = TRUE\n)\n\nspain_institutions |>\n  slice_max(cited_by_count, n = 8) |>\n  mutate(display_name = forcats::fct_reorder(display_name, cited_by_count)) |>\n  ggplot() +\n  aes(x = cited_by_count, y = display_name, fill = display_name) +\n  geom_col() +\n  scale_fill_viridis_d(option = \"E\") +\n  guides(fill = \"none\") +\n  labs(\n    x = \"Total citations\", y = NULL,\n    title = \"Spanish references\"\n  ) +\n  coord_cartesian(expand = FALSE)\n```\n:::\n\n\n\n### aRxiv\n\n[aRxiv](https://docs.ropensci.org/aRxiv/index.html) es un paquete de R para acceder a la API de arXiv. [arXiv](https://arxiv.org/) es un repositorio de acceso abierto de preprints de artículos científicos en física, matemáticas, ciencias de la computación, biología cuantitativa, finanzas cuantitativas, estadísticas y economía.\n\nEste paquete no lo he usado, ni siquiera he jugado un poco con él.\n\n\n### Busqueda de G-Schollar\n\n[Aquí](https://github.com/j-5chneider/sysreview/blob/master/scrape_google_scholar/scrape_google_scholar.R), gracias a [Johannes Schneider](https://scicomm.xyz/@jschneider), encontramos un script para escrapear una busqueda en G schollar. Tiene el código abajo.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest)\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(rio)\n\n############################################################################## #\n###                                                                            #\n### PROVIDE YOUR SETTINGS HERE                                              ####\n### Please fill in your specifications and then run the code                   #\n###                                                                            #\n############################################################################## #\n\n## Insert Search String here!\n# For tips on how to create a good search string for google scholar check out\n# http://musingsaboutlibrarianship.blogspot.com/2015/10/6-common-misconceptions-when-doing.html\nsearchstring <- 'intitle:(\"OPEN SCIENCE BADGES\" & \"TRUST\" & \"SCIENTISTS\")'\n\n## Which range of results should be exported?\n# Please provide in increments of 10\n# (otherwise this will be enforced, the script can only export entire pages of 10)\nfrom_result <- 1\nto_result <- 100\n\n\n\n############################################################################## #\n###                                                                            #\n### ONLY RUN THIS CODE                                                      ####\n### Without changing anything                                                  #\n###                                                                            #\n############################################################################## #\n\n# create empty object to save results\nreferences <- data.frame(authors = as.character(),\n                         year = as.character(),\n                         title = as.character(),\n                         journal = as.character(),\n                         abstract = as.character())\n\nfor (i in (seq(from = from_result, to = to_result, by = 10)-1)) { # a loop to scrape from several pages\n  # create URL of google scholar result page\n  url <- URLencode(paste0(\"https://scholar.google.com/scholar?start=\", \n                          i,              # indicates page\n                          \"&as_vis=1&q=\", # excludes citations (checkbox on the left)\n                          searchstring),  # passes the search string\n                   reserved = F)    # makes sure special characters are not encoded\n  \n  # scrape this juicy results page\n  page <- read_html(url) \n  Sys.sleep(1)\n  \n  for (j in 1:10) { # loop over all 10 results\n    # extract certain details from the result\n    references <- references %>% \n      add_row(authors = gsub(\"^(.*?)\\\\W+-\\\\W+.*\", \"\\\\1\", \n                             rvest::html_text(rvest::html_elements(page, \".gs_a\")), \n                             perl = TRUE)[j],\n              year = ifelse(str_detect(rvest::html_text(rvest::html_elements(page, \".gs_a\"))[j],\n                                       \"(\\\\d{4})\"),   # if year is detected\n                            gsub(\"^.*(\\\\d{4}).*\", \"\\\\1\", # then extract year\n                                 rvest::html_text(rvest::html_elements(page, \".gs_a\")),\n                                 perl = TRUE)[j],\n                            as.character(NA)),        # else missing value\n              title = rvest::html_text(rvest::html_elements(page, \".gs_rt\"))[j],\n              journal = ifelse(str_detect(rvest::html_text(rvest::html_elements(page, \".gs_a\"))[j],\n                                          \"(\\\\d{4})\"), # if year is detected\n                               gsub(\"^.*((?<=-\\\\s)(.*)(?=,+)).*\", \"\\\\1\",  # then\n                                    rvest::html_text(rvest::html_elements(page, \".gs_a\")),\n                                    perl = TRUE)[j],\n                               gsub(\"^.*((?<=-\\\\s)(.*)).*$\", \"\\\\1\",       # else\n                                    rvest::html_text(rvest::html_elements(page, \".gs_a\")),\n                                    perl = TRUE)[j]),\n              abstract = rvest::html_text(rvest::html_nodes(page, \".gs_rs\"))[j]\n      )\n  }\n}\n\n\n# clean up the messy titles\nreferences <- references |>\n  dplyr::mutate(title = stringr::str_replace_all(\n    title,\n    \"(\\\\[PDF\\\\]\\\\[PDF\\\\]\\\\s|\\\\[HTML\\\\]\\\\[HTML\\\\]\\\\s|\\\\[BUCH\\\\]\\\\[B\\\\]\\\\s)\", \n    \"\"))\n\n## Export the data set to the working directory as CSV\n# This CSV is compatible for import to Rayyan and ASReview\n# rio::export(references, \"references_googleScholar.csv\")\n```\n:::\n\n\n\n### G-Schollar rank of co-authors\n\nEn este [post](https://quantixed.org/2023/10/21/all-the-right-friends-how-does-google-scholar-rank-co-authors/) hace uso del paquete [scholar](https://github.com/jkeirstead/scholar) para intentar comprender como se calcula el ranking de co-autores en G-Schollar. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(scholar)\n\nid <- 'PXaWcW4AAAAJ' #- tú ID de G-Schollar\n\nmy_profile <- get_profile(id)\n\nmy_citation_h <- get_citation_history(id)\n\nmy_pubs <- get_publications(id)\n```\n:::\n\n\n\n## Otros\n\n\n### Qualtrics pkg\n\n[Qualtrics](https://www.qualtrics.com/es/) es una plataforma online para realizar encuestas. \n[qualtRics](https://docs.ropensci.org/qualtRics/) es un paquete de R para trabajar con la API de Qualtrics. Permite descargar datos de encuestas, crear encuestas, modificarlas, etc. \n\n\n### Ecuaciones de Word\n\nCon `pandoc` es fácil transformar un archivo `.docx` a Quuarto, pero si sólo quieres coger una ecuación hecha en Word y pegarla en Quarto, puedes hacerlo con [este](https://jgostick.github.io/mml2latex/)  MathML to LaTeX Converter.\n\n### SPSS\n\nEl paquete [expss](https://cran.r-project.org/web/packages/expss/index.html) permite generar tablas al estilo SPSS. El paquete [r2spss](https://cran.r-project.org/web/packages/r2spss/vignettes/r2spss-intro.pdf)  allows to create plots and LaTeX tables that look like SPSS\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(expss)\ndata(mtcars)\nmtcars = apply_labels(mtcars,\n                      mpg = \"Miles/(US) gallon\",\n                      cyl = \"Number of cylinders\",\n                      disp = \"Displacement (cu.in.)\",\n                      hp = \"Gross horsepower\",\n                      drat = \"Rear axle ratio\",\n                      wt = \"Weight (1000 lbs)\",\n                      qsec = \"1/4 mile time\",\n                      vs = \"Engine\",\n                      vs = c(\"V-engine\" = 0,\n                             \"Straight engine\" = 1),\n                      am = \"Transmission\",\n                      am = c(\"Automatic\" = 0,\n                             \"Manual\"=1),\n                      gear = \"Number of forward gears\",\n                      carb = \"Number of carburetors\"\n)\n\ncross_cases(mtcars, am, vs)\n```\n:::\n\n\n## SAS\n\nEl paquete [procs](https://procs.r-sassy.org/) permite obtener tablasy resultados similares a los obtenidos en SAS. En realidad el paquet forma parte de [sassy](https://r-sassy.org/) un ecosistema, unn integrated set of packages designed to make programmers more productive in R, particularly those with a background in SAS® software\n\nPb, solo sale en el Viewer de RStudio, no en QMD\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(procs)\n\n# Turn off printing for CRAN checks\noptions(\"procs.print\" = TRUE)\n\n# Prepare sample data\ndt <- as.data.frame(HairEyeColor, stringsAsFactors = FALSE)\n\n# Assign labels\nlabels(dt) <- list(Hair = \"Hair Color\",\n                   Eye = \"Eye Color\")\n\n# Produce frequency statistics\nres <- proc_freq(dt, tables = v(Hair, Eye, Hair * Eye),\n                 weight = Freq,\n                 output = report,\n                 options = chisq,\n                 titles = \"Hair and Eye Frequency Statistics\")\n\nres\n```\n:::\n\n\n\n## Grafico con la ruta a archivos\n\n[dir](https://emitanaka.org/dir/index.html) es un paquete de Emi Tanaka para visualizar la estructura de directorios. Otra alternativa es el paquete [jsTree](https://yonicd.github.io/jsTree/)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndir::listing()\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}